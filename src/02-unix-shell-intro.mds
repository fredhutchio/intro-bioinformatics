% Introduction to Bioinformatics
% Lesson 2 - Unix Foundations

# The Shell

## Shell : Bioinformatician &nbsp; :: &nbsp; Water : Fish

<br/>

* Philosophy
* Tools
* Cluster computing


# The Unix Philosophy

* Small, composable tools that do one thing right
* Plumb tools together into pipelines/scripts
* Embrace plain text/data

This gives us a lot of power.


# Let's break things down a bit


# Directories, files and paths, oh my!

In which we see the static world of things.


# In Unix, (almost) everything is a file

* programs
* devices
* /dev/null
* etc.


# And files have **paths**

Paths are the addresses for files, and are based on _directories_.


# Directories (aka "folders") are containers

## ...for more directories and files...


# Some special paths & directories:

* `/` - The "root" of the filesystem; everything lives here
* `.` - Current directory (depends on where you are)
* `..` - Parent directory (depends on where you are)
* `~` - Home directory


# How paths work

We have a directory called `bioinfclass` in the directory `~`.
The path to this directory is `~/bioinfclass`.

`bioinfclass` also has a subdirectory called `data`, so it's path is `~/bioinfclass/data`.


# Path tools

* change directories with `cd`
* list directory contents with `ls`
* see where we are with `pwd`


# Example: Using path tools

```bash
cd ~

pwd

# list the contents of this directory
ls bioinfclass/data

cd bioinfclass

pwd

ls data
```


# Exercise: going up the directory chain

Try

```bash
ls ..

ls ../..

cd /
```

**Excecise:** Where are we now? How can we get back to our home directory?


# We can change the path of a file using `mv`

```bash
mv bioinfclass/data foobar

ls

ls foobar

ls bioinfclass
```

**Excercise:** How do we get `data` back?


# `mv` is for renaming as well

If the directory of the file is the same, we just _rename_ the file, but the command stays the same.

```bash
mv bioinfclass foobar

ls

mv foobar bioinfclass
```


# We can make new directories or files with `mkdir` and `touch`

* `mkdir name` will create a directory `name`
* `touch name` will create an empty file `name`

&nbsp;

**Exercise:** create a file in a subdirectory from your home directory


# Commands

In which the things take flight and evolve!


# Anatomy of a Unix command

`command [flags] [operands]`

* command: name of the program
* operands: positional based arguments
    * often but not always required
* flags: named based arguments
    * position frequently ignored
    * usually for controlling details of program behavior
    * often not required
    * sometimes take arguments, sometimes not


# More on flag shape

* sometimes single dash and character (like `-h`)
* sometimes double dash and word (like `--help`)
* sometimes single dash and word (like `-help`; esp Java programs)
* sometimes there are short and long options (like `-h` or `--help`)
* sometimes short flags can be munged together (`-v -x -f the-file` == `-vxf the-file`)


# Help with flags

```bash
ls --help

```


# Getting help with `man` and `apropos`

There is built in help on the command line:

* Try `man ls` (arrows to scroll, q to quit)
* Try `apropos calculator`


# Programs are just files!

```bash
which tree

# -l: "long" list output; lots of info that we will ignore except for `x` presence/absence
ls -l /usr/bin/tree

# super secret shortcut
ls -l $(which tree)

touch testfile

# Notice no x; not executable...
ls -l testfile
```


# What else is in `/usr/bin`?

`ls /usr/bin` => LOTS OF STUFF!

This is one of the many places your shell looks for programs


# How does the shell know where to look for programs?

`echo $PATH` => `:` separated list of places your shell looks for programs.

This can be edited if you want to let your shell know to look other places as well (see `.bashrc`)


# Pipes and streams

In which our constructions in flight don tongues and communicate!


# A bit about `std` files

There are three special files that programs often read from and write to by default to facilitate composability:

^imghl "figures/stdfiles.svg" 300

For the most part we'll ignore `stderr`...


# By default, stdout gets printed to the screen

## But we can _redirect_ stdout to a file using `>`

`Program` vs `Program > out-file`

^imghl "figures/redirect.svg" 300


# Example of redirection

`FastTree -nt output/alignment.fasta`

vs

`FastTree -nt output/alignment.fasta > output/tree.nw`


&nbsp;


[if your shell can't see FastTree, \
don't forget to `module load intro-bio`!]



# We can also `|` `stdout` of one command to `stdin` of another

`Prog1 | Prog2`:

^imghl "figures/pipe.svg" 300


# Example

`column -t -s , data/sfv.csv | less -S`

* `column` writes it's output to `stdout`
    * `-t` specifies tabs for output
    * `-s ,` specifies command for separator
* `less` reads from `/dev/stdin` if no file is specified
    * `-S` specifies don't wrap


#
Programs have to _know_ to use stdin / stdout for this to work.
Some conventions:

* Assume first argument is input, second output
* If only one argument is specified, output to `stdout`
* If no arguments specified, input from `stdin`, output to `stdout`
* Sometimes there is no "output" argument; assume `stdout`
* Sometimes programs treat `-` as a shortcut for `stdin`
* When all else fails, you can read from & write to `/dev/stdout`, `/dev/stdin` directly


# Practicum!

In which we apply our knowledge of these flying, speaking beasts.


# First, let's look at our CSV data a bit more

We'll be using several programs with `stdin`, `stdout` and redirection to answer questions about our data.


# Side note: strategy

The way to work through and understand these examples:

* run the first part of the command and think about what it's doing
* add the next part of the pipe to see what that does
* repeat, till we've done the whole command
* reflect and think about the strategy used to put each of these pieces together towards the end goal

This is useful in part because this is typically how we write.


# Head first...

Head writes to `stdout` the first several lines of `stdin`.

```bash
head data/sfv.csv

column -t -s , data/sfv.csv | head
```

This is useful for working with big files.


# Let's take a look at our alignment from last week

## Always do this with alignments!

```bash
# We'll use the `alnvu` program
av -L 99999 -w 99999 -c output/alignment.fasta | less -S
```


# We can also look at our tree

```bash
nw_display output/tree.nw | less -S
```


# So we can look at data in Unix

But how can we actually analyze data and answer questions?


# How many human sequences are there?

There is a species column in our data set with `human` and `monkey` entries.

<br/>

How would we figure out how many human and monkey sequences there are in the data?


# How many human sequences are there?


```bash
grep human data/sfv.csv | wc -l

# what about monkey sequences?
grep monkey data/sfv.csv | wc -l
```

* `grep` searches through files for a pattern
* `wc -l` counts the number of lines
* Thus, `grep ... | wc -l` counts the number of lines in a file matching some pattern


# Some problems with this:

We're not accounting for the tabular structure:

* We can't restrict matches to just a particular column in the data
* We have to do this twice; if we had many categories, this could take forever


# A better way

How many sequences are there for each species?

```bash
cut -d , -f 3 data/sfv.csv | sort | uniq -c
```

* `cut -d , -f 3` extracts the third "," separated entry out of each line
* `sort | uniq -c` makes a count of occurances for each unique entry


# How to visualize this

The first few columns in the data

    |-----------------+----------+---------+------+-------------|
    |  sequence       | specimen | species | gene | location    |
    |-----------------+----------+---------+------+-------------|
    |  BGH150WBGag2   | BGH150   | human   | gag  | Charmaguria |
    |  BGH31WBGag2    | BGH31    | human   | gag  | Bormi       |
    |  BGH150WBGag4   | BGH150   | human   | gag  | Charmaguria |
    |  BGH150WBGag8   | BGH150   | human   | gag  | Charmaguria |
    |  MBG103WBGag101 | MBG103   | monkey  | gag  | Narayanganj |
    |  BGH31WBGag3    | BGH31    | human   | gag  | Bormi       |
    |  MBG103WBGag102 | MBG103   | monkey  | gag  | Narayanganj |
    |  BGH31WBGag1    | BGH31    | human   | gag  | Bormi       |
    |  MBG103WBGag104 | MBG103   | monkey  | gag  | Narayanganj |
    |  MBG104WBGag101 | MBG104   | monkey  | gag  | Narayanganj |
    |  ...

# The results of cutting out the third column

`cut -d , -f 3 data/sfv.csv`

    |---------|
    | species |
    |---------|
    | human   |
    | human   |
    | human   |
    | human   |
    | monkey  |
    | human   |
    | monkey  |
    | human   |
    | monkey  |
    | monkey  |
    | ...


# The result of sorting

`cut -d , -f 3 data/sfv.csv | sort`

    |---------|
    | species |
    |---------|
    | human   |
    | human   |
    | human   |
    | human   |
    | human   |
    | human   |
    | ...
    | monkey  |
    | monkey  |
    | monkey  |
    | monkey  |
    | ...


# The result of counting

`cut -d , -f 3 data/sfv.csv | sort | uniq -c`

      77 human
    1097 monkey
       1 species


# How would we count how many specimens there are per species?


# How about how many specimens there are per species?

We can do this with a two step version of the `cut | uniq | sort` pattern we used above:

```bash
# First, uniq specimens, with species info
cut -d , -f 2,3 data/sfv.csv | sort | uniq | head

# Next count that by species
cut -d , -f 2,3 data/sfv.csv | sort | uniq | cut -d , -f 2 | sort | uniq -c
```


# Columns 2 and 3 cut out of data

`cut -d , -f 2,3 data/sfv.csv`

    |----------+---------|
    | specimen | species |
    |----------+---------|
    | BGH150   | human   |
    | BGH31    | human   |
    | BGH150   | human   |
    | BGH150   | human   |
    | MBG103   | monkey  |
    | BGH31    | human   |
    | MBG103   | monkey  |
    | BGH31    | human   |
    | MBG103   | monkey  |
    | MBG104   | monkey  |
    | ...


# The results of the first sort and uniq (without `-c`)

`cut -d , -f 2,3 data/sfv.csv | sort | uniq`

    |----------+---------|
    | specimen | species |
    |----------+---------|
    | BGH150   | human   |
    | BGH31    | human   |
    | MBG103   | monkey  |
    | MBG104   | monkey  |
    | ...

Now we now have exactly 1 row per specimen (species just comes along for the ride).


# Now we cut out the species, and count

`cut -d , -f 2,3 data/sfv.csv | sort | uniq | cut -d , -f 2 | sort | uniq -c`

      8 human
    169 monkey
      1 species

There is exactly one row in the last result set for each unique specimen.
Thus, the number of times we see a species in that table is the number of specimens corresponding to that species.


# What if we wanted to include location in this data?

What would we do?


# What about specimens by location?

```bash
cut -d , -f 2,3,5 data/sfv.csv | sort | uniq | cut -d , -f 2,3 | sort | uniq -c
```


# Moral of the story

Small composable tools offer a lot of power and flexibility.


# Is anyone else getting tired of all this `-d , -f 2,3,5` business?

## Enter `csvkit`

We have `csv...` versions of many of the standard Unix utils:

`csvcut`,
`csvgrep`,
`csvjoin`,
`csvlook`,
`csvsort`,
`csvstack`,
`csvstat`


# Advantages of csvkit

* Can use header names, not just indices
* Don't have to specify `-s ,`/`-d ,` every time
* Headers are treated separately from the data


# Looking at just the human sequences

```bash
csvgrep -c species -m human data/sfv.csv | csvlook | less -S
```


# The one missing piece: csvuniq

```bash
# ~/bin is one of the places we can put programs
mkdir ~/bin

wget https://goo.gl/ZdNoUL -O ~/bin/csvuniq

# Mark as executable, and add ~/bin to PATH
chmod +x ~/bin/csvuniq
export PATH=~/bin:$PATH

# Have to install some dependencies
R
install.packages(c('optparse', 'plyr'))
# Enter y at all prompts, and select the WA CRAN mirror when asked.
# When done, exit with
quit()
```


# Now let's see what our example looks like:

```bash
csvuniq -c specimen,species,location data/sfv.csv | csvuniq -zc species,location | csvlook
```

Notice that `csvuniq` does all of the sorting and cutting for us...


# Homework

For homework go over these examples, and translate into the equivalents using `csvkit` utils.

Recommended reading:

* Chapter 3, and chapter 7 (till around page 148).

Reading for next class (if you want to read ahead):

* Chapter 12


# Resources

* Unix command reference: <https://ubuntudanmark.dk/filer/fwunixref.pdf>

[Back to homepage](http://fredhutchio.github.io/intro-bioinformatics)


