% Introduction to Bioinformatics
% Lesson 3 - Unix Scripting & Automation


# First some housekeeping:

* Questions from last week?
* Recover last week's pipe example?


# Shell scripting

_In which our flying, speaking beasts assemble and become legion_


# The Unix shell is more than just a shell

It's also a mini-programming language!

Building new commands out of old commands is part of Unix composability


# For bioinformatics this means

* Simple analysis tools/commands
* Automation


# Shell scripting is just "duct tape"

* Perfect for connecting existing shell commands together
* Less so for more complicated data flow and logic (for which we have python, R, etc.)


# Our first script!

Let's start by creating a build script that automates some of the things we've done so far


# A shell script is just a list of commands to be executed

In your `build.sh` file, try:

```
ls -l | column -t
echo "IT WORKED!"
```

Then run `bash build.sh` from within the `bioinfclass` directory.

This tells the shell to run your script.


# A more complete example

In your `build.sh` file:
```
# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
# Put in a "bug":
asdfasdfasdf

echo "IT WORKED!"
```
Lines that start with `# ` are comments and aren't executed. <br/>
Again try `bash build.sh`.
Then remove the "bug" and run.

# Let's make the script more of a proper command

In your `build.sh` file:

```
#!/bin/bash

# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
echo "IT WORKED!"
```

The `#!` is a "shebang": tells computer how to run script


# Now make the script executable!

At the terminal:

```
chmod +x build.sh

# Note "x" when we use `ls -l`
ls -l build.sh

file build.sh

# Now it's a command/program!
./build.sh
```

# Taking a step back to optimize our environment with dotfiles

We've already seen dotfiles in use with Tmux.
Many other Unix programs accept configuration from dotfiles, including `nano` and our Unix shell itself.


# First some things to make nano nicer

Run `nano ~/.nanorc`, then enter:

```
set tabsize 4
set tabstospaces
set autoindent
```

It's not perfect, but it's a start.


# Now our environment

Tired of running `module load intro-bio`?
Or editing your `PATH` variable?
`nano ~/.bashrc`:

```
module load intro-bio
export PATH=~/bin/:$PATH
```

Now run `source ~/.bashrc` to reload.


# Back to biology


# Counting sequences per species

In our `build.sh` file:

```
# Compute number of sequences per species
csvuniq -zc species data/sfv.csv > output/seqs_per_species.csv
```

Type `Ctrl-o` to save without exiting.
Then `Ctrl-a c` to start a new tmux window.


# Run and investigate

In the shell:

```
ls output

./build.sh

ls output

csvlook output/seqs_per_species.csv
```

Then `Ctrl-a <Space>` to switch back to the last tmux window.


# Add some other steps

In our `build.sh` file:

```
# Compute number of sequences per specimen
csvuniq -zc specimen,species,location data/sfv.csv > output/seqs_per_specimen.csv

# Use those results to count specimens per species
csvuniq -zc species output/seqs_per_specimen.csv > output/specs_per_species.csv

# Also use them to count specimens by species and location
csvuniq -zc species,location output/seqs_per_specimen.csv > output/specs_per_species_location.csv
```


# Run and investigate

In the shell:

```
./build.sh

csvlook output/seqs_per_specimen.csv | less -S
```


# How shell variables work

To clarify and simplify, we introduce _variables_!

In the shell:

```
# Assign the value "output" to the variable outdir
# Note no spaces around the "="
outdir="output"

# We can use that variable in further commands by prepending $
echo $outdir
ls $outdir
```


# Cleaning things up with variables

In our `build.sh` file:

```
# Specify output directory, and name input data
outdir="output"
metadata="data/sfv.csv"


# Compute number of sequences per species
seqs_per_species="$outdir/seqs_per_species.csv"
csvuniq -zc species $metadata > $seqs_per_species

# Compute number of sequences per specimen
seqs_per_specimen="$outdir/seqs_per_specimen.csv"
csvuniq -zc specimen,species,location $metadata > $seqs_per_specimen
```


# Continuing variable clean up

## Using output variables as input:

```
# Use those results to compute number of specimens per species
specs_per_species="$outdir/specs_per_species.csv"
csvuniq -zc species $seqs_per_specimen > $specs_per_species

# Also use them to compute number of specimens per species and location
specs_per_species_location="$outdir/specs_per_species_location.csv"
csvuniq -zc species,location $seqs_per_specimen > $specs_per_species_location
```

<!-- 18 minutes so far -->


# Verify that it runs

```bash
./build.sh

ls output

csvlook output/seqs_per_specimen.csv
```


# Writing your own commands

## The other side of shell scripting, and fulfillment of the dream of composition


# Looking for common command patterns

## Letting laziness lead the way...

We've seen `csvlook ___ | less -S` quite a few times now for looking at csv data.
So let's give this a name!


# Writing a `csvless` script

Open up a new file in `~/bin/csvless`, and write:

```
#!/bin/bash

csvlook $@ | less -S
```

Here `$@` is a "magic" variable that points to all of the arguments passed to the command.


# Testing our `csvless` script

In the shell:

```
chmod +x ~/bin/csvless

csvless data/sfv.csv

csvcut -c sequence,specimen,species | csvless
```


# Another quick example: `csvhead`

Open up a new file in `~/bin/csvhead`, and write:

```
#!/bin/bash

csvlook $@ | head -n 20
```

Again, `chmod +x`, then test it!


# Other things to know for shell scripting:

* Iteration and arrays: Let us nest or loop over things like different species or locations and process separately
* Conditionals: Let you run tests to determine what code to run
* find/xargs/parallel: Run a command or program in parallel over a sequence of files

Most of this we'll leave to the book...


# Simple iteration

```
for i in $(ls); do echo "My file $i is cool"; done
```

# Conditionals

```
#!/bin/bash

if [commands]
then
  [if-statements]
else
  [else-statements]
fi
```


# Conditional example

```
#!/bin/bash

if grep "pattern" some_file.txt > /dev/null
then
  # commands to run if "pattern" is found
  echo "found 'pattern' in 'some_file.txt"
else
  echo "no such pattern found..."
fi
```


# find/xargs/parallel

There is lots of explanation in the book, so you can learn more there. But to get a brief sense:

* `find`: Let's you compute very specific lists of files, returned to `stdout` separated by lines
* `xargs`: Can take line separated items, and apply them as arguments to some command
    * Good for large collections
    * Can run in parallel
* `parallel`: More robust version of xargs, with better control over parallelization


# Build tools such as `make` offer the following advantages over shell scripts:

* Only rebuild what's needed (very important with long running programs) by tracking:
    * whether data has changed
    * whether commands have changed
* Generally more robust
* Automatically parallelize (sanely)


# Excercise 1

Let's add the following to the end of our `build.sh` script:

```
#!/bin/bash

# ...

# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # Create a location outdir, if it doesn't already exist
  loc_outdir="$outdir/$location"
  mkdir -p $loc_outdir

  # Create a subset of the metadata for just that location
  loc_metadata="$loc_outdir/metadata.csv"
  csvgrep -c location -m $location $metadata > $loc_metadata

  # Create a list of sequences sampled from that location
  loc_sequences="$loc_outdir/sequences"
  csvcut -c sequence $loc_metadata > $loc_sequences

  # Do something interesting with each location's sequences, etc
  # ...
done

# Do something interesting with the things done for each location
# ...
```


# Other exercises

* Fill in our little `build.sh` script with some of the other things we've done, like the alignment and tree building.
* Write a handy little shell script for doing something (for example, printing the last 20 commands entered)
* Flesh out the for loop by
    * Subset the main alignment to each location using `seqmagick convert`
    * Build a tree from that subset alignment

Don't forget to read through the book!


# Reading

Recommended reading:

* Chapter 12

Reading for next class (if you want to read ahead):

* Chapter 5


# Resources

* SCons (sane `make` alternative) for bioinformatics post - <http://www.metasoarous.com/scons-for-data-science-and-compbio/>
* seqmagick - <http://fhcrc.github.io/seqmagick/> for munging sequence data
* `nano` for simple file editing on the rhinos
* Vim for more powerful text editing - (type `vimtutor` at the terminal...)
* Erick's favorite [video](https://www.youtube.com/watch?v=olH-9b3VJfs) about shell scripting, and [website](http://shellhaters.org/)


# SPOILER ALERT!!!

## The following slides contain solutions to the exercises.

Go no further if you're keen to solve the problems yourself.


# Automating alignment and tree building

```
#!/bin/bash
# ...

inseqs="data/sfv.fasta"

# Alignment
alignment="$outdir/alignment.fasta"
muscle -maxiters 2 -in $inseqs -out $alignment

# Tree
tree="$outdir/tree.nw"
FastTree -nt $alignment > $tree
```


# The for loop thing

```
  # Subset our alignment to just that location
  loc_alignment="$loc_outdir/alignment.fasta"
  seqmagick convert --include-from-file $loc_sequences $alignment $loc_alignment

  # Build a location tree
  loc_tree="$loc_outdir/tree.nw"
  FastTree -nt $loc_alignment
```


# This slide intentionally left almost blank...

[Back to homepage](http://fredhutchio.github.io/intro-bioinformatics)

