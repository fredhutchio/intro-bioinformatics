% Introduction to Bioinformatics
% Lesson 3 - Unix Scripting & Automation


# First some housekeeping:

* Sharing emails for support?
* Recover last week's pipe example?
* Have you been using the resources?
* How has reading the book been?
* You are all awesome; thanks for bearing with us...


# Shell scripting

_In which our flying, speaking beasts assemble and become legion_


# The Unix shell is more than just a shell

It's also a mini-programming language!

Building new commands out of old commands is part of Unix composability


# For bioinformatics this means

* Simple analysis tools/commands
* Automation


# Shell scripting is just "duct tape"

* Perfect for connecting existing shell commands together
* Less so for more complicated data flow and logic (for which we have python, R, etc.)


# Our first script!

Let's start by creating a build script that automates some of the things we've done so far


# Editing text files on the server

* Reconnect to your favorite rhino, `tmux attach`, load modules if needed, etc.
* Click the square on the top left of the NoMachine window
* Type in Gedit, and hit enter
* Click the "Save" button
* Save to `~/bioinfclass/build.sh`

The home directory on NoMachine is the same as your home directory on the rhinos.

(PS This is where you can use `sshfs` on a mac/Linux computer or `vim`/`emacs` directly on the server if you wish)


# A shell script is just a list of commands to be executed

In your `build.sh` file, try:

```
ls -l | column -t
echo "IT WORKED!"
```

Then run `bash build.sh` from within the `bioinfclass` directory.

This tells the shell to run your script.


# A more complete example

In your `build.sh` file:

```
# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
# Put in a "bug":
asdfasdfasdf

echo "IT WORKED!"
```
Lines that start with `# ` are comments and aren't executed.

Again run `bash build.sh` and see what happens.
Then remove the "bug" and run again.

# Let's make the script more of a proper command

In your `build.sh` file:

```
#!/bin/bash

# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
echo "IT WORKED!"
```

The `#!` is a "shebang": tells computer how to run script


# Now make the script executable!

At the terminal:

```
chmod +x build.sh

# Note "x" when we use `ls -l`
ls -l build.sh

file build.sh

# Now it's a command/program!
./build.sh
```


# Counting sequences per species

In our `build.sh` file:

```
# Compute number of sequences per species
csvuniq -zc species data/sfv.csv > output/seqs_per_species.csv
```


# Run and investigate

In the shell:

```
ls output

./build.sh

ls output

csvlook output/seqs_per_species.csv
```


# Add some other steps

In our `build.sh` file:

```
# Compute number of sequences per specimen
csvuniq -zc specimen,species,location data/sfv.csv > output/seqs_per_specimen.csv

# Use those results to count specimens per species
csvuniq -zc species output/seqs_per_specimen.csv > output/specs_per_species.csv

# Also use them to count specimens by species and location
csvuniq -zc species,location output/seqs_per_specimen.csv > output/specs_per_species_location.csv
```


# Run and investigate

In the shell:

```
./build.sh

csvlook output/seqs_per_specimen.csv | less -S
```

# How shell variables work

To clarify and simplify, we introduce _variables_!

In the shell:

```
# Assign the value "output" to the variable outdir
# Note no spaces around the "="
outdir="output"

# We can use that variable in further commands by prepending $
echo $outdir
ls $outdir
```


# Cleaning things up with variables

In our `build.sh` file:

```
# Specify output directory, and name input data
outdir="output"
metadata="data/sfv.csv"


# Compute number of sequences per species
seqs_per_species="$outdir/seqs_per_species.csv"
csvuniq -zc species $metadata > $seqs_per_species

# Compute number of sequences per specimen
seqs_per_specimen="$outdir/seqs_per_specimen.csv"
csvuniq -zc specimen,species,location $metadata > $seqs_per_specimen
```


# Continuing variable clean up

## Using output variables as input:

```
# Use those results to compute number of specimens per species
specs_per_species="$outdir/specs_per_species.csv"
csvuniq -zc species $seqs_per_specimen > $specs_per_species

# Also use them to compute number of specimens per species and location
specs_per_species_location="$outdir/specs_per_species_location.csv"
csvuniq -zc species,location $seqs_per_specimen > $specs_per_species_location
```

<!-- 18 minutes so far -->


# Verify that it runs

```
./build.sh

ls output

csvlook output/seqs_per_specimen.csv
```


# Writing your own commands

## The other side of shell scripting, and fulfillment of the dream of composition


# Looking for common command patterns

## Letting laziness lead the way...

We've seen `csvlook ___ | less -S` quite a few times now for looking at csv data.
So let's give this a name!


# Writing a `csvless` script

Open up a new file in `~/bin/csvless`, and write:

```
#!/bin/bash

csvlook $@ | less -S
```

Here `$@` is a "magic" variable that points to all of the arguments passed to the command.


# Testing our `csvless` script

In the shell:

```
export PATH=~/bin:$PATH

chmod +x ~/bin/csvless

csvless data/sfv.csv

csvcut -c sequence,specimen,species | csvless
```


# Another quick example: `csvhead`

Open up a new file in `~/bin/csvhead`, and write:

```
#!/bin/bash

csvlook $@ | head -n 20
```

Again, `chmod +x`, then test it!


# Other things to know for shell scripting:

* Iteration and arrays: Let us nest or loop over things like different species or locations and process separately
* Conditionals: Let you run tests to determine what code to run
* find/xargs/parallel: Run a command or program in parallel over a sequence of files

Most of this we'll leave to the book...


# Simple iteration

```
for i in $(ls); do echo "My file $i is cool"; done
```

# Conditionals

```
#!/bin/bash

if [commands]
then
  [if-statements]
else
  [else-statements]
fi
```


# Conditional example

```
#!/bin/bash

if grep "pattern" some_file.txt > /dev/null
then
  # commands to run if "pattern" is found
  echo "found 'pattern' in 'some_file.txt"
else
  echo "no such pattern found..."
fi
```


# find/xargs/parallel

There is lots of explanation in the book, so you can learn more there. But to get a brief sense:

* `find`: Let's you compute very specific lists of files, returned to `stdout` separated by lines
* `xargs`: Can take line separated items, and apply them as arguments to some command
    * Good for large collections
    * Can run in parallel
* `parallel`: More robust version of xargs, with better control over parallelization


# Build tools such as `make` offer the following advantages over shell scripts:

* Only rebuild what's needed (very important with long running programs) by tracking:
    * whether data has changed
    * whether commands have changed
* Generally more robust
* Automatically parallelize (sanely)


# Excercise 1

Let's add the following to the end of our `build.sh` script:

```
#!/bin/bash

# ...

# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # Create a location outdir, if it doesn't already exist
  loc_outdir="$outdir/$location"
  mkdir -p $loc_outdir

  # Create a subset of the metadata for just that location
  loc_metadata="$loc_outdir/metadata.csv"
  csvgrep -c location -m $location $metadata > $loc_metadata

  # Create a list of sequences sampled from that location
  loc_sequences="$loc_outdir/sequences"
  csvcut -c sequence $loc_metadata > $loc_sequences

  # Do something interesting with each location's sequences, etc
  # ...
done

# Do something interesting with the things done for each location
# ...
```


# Other exercises

* Fill in our little `build.sh` script with some of the other things we've done, like the alignment and tree building.
* Write a handy little shell script for doing something (for example, printing the last 20 commands entered)
* Flesh out the for loop by
    * Subset the main alignment to each location using `seqmagick convert`
    * Build a tree from that subset alignment

Don't forget to read through the book!


# Resources

* SCons (sane `make` alternative) for bioinformatics post - <http://www.metasoarous.com/scons-for-data-science-and-compbio/>
* seqmagick - <http://fhcrc.github.io/seqmagick/> for munging sequence data
* `nano` for simple file editing on the rhinos
* Vim for very powerful though obscure file editing - (just type `vimtutor` at the terminal...)
* Erick's favorite [video](https://www.youtube.com/watch?v=olH-9b3VJfs) about shell scripting, and [website](http://shellhaters.org/)

# SPOILER ALERT!!!

## The following slides contain solutions to the exercises.

Go no further if you're keen to solve the problems yourself.


# Automating alignment and tree building

```
#!/bin/bash
# ...

inseqs="data/sfv.fasta"

# Alignment
alignment="$outdir/alignment.fasta"
muscle -maxiters 2 -in $inseqs -out $alignment

# Tree
tree="$outdir/tree.nw"
FastTree -nt $alignment > $tree
```


# The for loop thing

```
  # Subset our alignment to just that location
  loc_alignment="$loc_outdir/alignment.fasta"
  seqmagick convert --include-from-file $loc_sequences $alignment $loc_alignment

  # Build a location tree
  loc_tree="$loc_outdir/tree.nw"
  FastTree -nt $loc_alignment
```


# This slide intentionally left almost blank...


