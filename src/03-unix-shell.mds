% Lesson 3 - Unix Scripting & Automation
% Introduction to Bioinformatics


# First things first

* Were you able to get `csvuniq` installed/working (check `~/bin/csvuniq -h`)?
* Note: `Ctrl-k` is useful for killing entire lines in nano
* You can turn off auto-indentation mode when pasting using `Esc i`


# Shell scripting

_In which our flying, speaking beasts assemble and become legion_


# The Unix shell is more than just a shell

It's also a mini-programming language!

Next to piping, this is our second route towards the promise of Unix composability


# For bioinformatics this means

* Simple analysis tools/commands
* Automation


# Shell scripting is just "duct tape"

* Perfect for connecting existing shell commands together
* Less so for more complicated data flow and logic (for which we have python, R, etc.)


# Our first script!

We're going to create a `build.sh` script to automate our analyses.


# A shell script is just a list of commands to be executed

Run `nano build.sh`, then add:

```
ls -l | column -t
echo "IT WORKED!"
```

* Save (`Ctrl-o`)
* Switch to a new t-mux pane (`Ctrl-a |`)
* Run `bash build.sh` from within the `bioinfclass` directory


# Error handling

Back in our `nano build.sh` pane, edit to look like:

```
# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
# Put in a "bug":
asdfasdfasdf

echo "IT WORKED!"
```

Save and rerun `bash build.sh`.
Remove the "bug" and rerun again.

Note: Lines starting with `# ` are comments and not executed.


# Making into a proper command

Edit your `nano build.sh` to look like:

```
#!/bin/bash

# Sane error handling settings
set -euf -o pipefail

ls -l | column -t
echo "IT WORKED!"
```

The `#!` is a "shebang": a special comment line that tells computer how to run our script


# Now make the script executable!

At the terminal:

```
chmod +x build.sh

# Note "x" when we use `ls -l`
ls -l build.sh

file build.sh

# Now it's a command/program!
./build.sh
```


# Taking a step back to optimize our environment with dotfiles

We've already seen dotfiles in use with Tmux.
Many other Unix programs accept configuration from dotfiles, including `nano` and our Unix shell itself.


# First some things to make nano nicer

Run `nano ~/.nanorc`, then enter:

```
set tabsize 4
set tabstospaces
set autoindent
```

It's not perfect, but it's a start.


# Now our environment

Tired of running `module load intro-bio`?
Or editing your `PATH` variable?

Run `nano ~/.bashrc`:

```
module load intro-bio
export PATH=~/bin/:$PATH
```

Now run `source ~/.bashrc` to reload.


# Back to biology


# Counting sequences per species

In our `build.sh` file, replace the `column -t` command with:

```
# Compute number of sequences per species
csvuniq -zc species data/sfv.csv > output/seqs_per_species.csv
```

Type `Ctrl-o` to save without exiting.


# Run and investigate

In the shell:

```
ls output

./build.sh

ls output

csvlook output/seqs_per_species.csv
```


# Add some other steps

In `nano build.sh` file, add to the end:

```
# Compute number of sequences per specimen
csvuniq -zc specimen,species,location data/sfv.csv > output/seqs_per_specimen.csv

# Use those results to count specimens per species
csvuniq -zc species output/seqs_per_specimen.csv > output/specs_per_species.csv

# Also use them to count specimens by species and location
csvuniq -zc species,location output/seqs_per_specimen.csv > output/specs_per_species_location.csv
```


# Run and investigate

In the shell:

```
./build.sh

csvlook output/seqs_per_specimen.csv | less -S
```

# Questions?


# Shell variables

Shell variables are an example of _indirection_.

They act as shorthand references to paths, files, or data.
What the variable name points to can change, without having to change (much) of the code.
This makes it easier to grow your scripts.


# Shell variables

In the shell:

```bash
# Set the shell variable `birds`
# (Note: no spaces around the "=")
birds="are really dinosaurs"

# We can use that variable in further commands by prepending $
echo $birds
```


# Cleaning up our build script

Edit our `build.sh` file:

```
# Specify output directory, and name input data
outdir="output"
metadata="data/sfv.csv"

# Compute number of sequences per species
seqs_per_species="$outdir/seqs_per_species.csv"
csvuniq -zc species $metadata > $seqs_per_species

# Compute number of sequences per specimen
seqs_per_specimen="$outdir/seqs_per_specimen.csv"
csvuniq -zc specimen,species,location $metadata > $seqs_per_specimen
```


# Cleaning up our build script

Add the following to the end of our build script.

```
# Use seqs_per_specimen to compute specimens per species
specs_per_species="$outdir/specs_per_species.csv"
csvuniq -zc species $seqs_per_specimen > $specs_per_species

# Also use them to compute number of specimens per species and location
specs_per_species_location="$outdir/specs_per_species_location.csv"
csvuniq -zc species,location $seqs_per_specimen > $specs_per_species_location
```

Note how much nicer it is referring to past results using shell variables?


# Verify that it runs

```bash
./build.sh

ls output

csvlook output/seqs_per_specimen.csv
```


# Questions


# Writing your own commands

## The other side of shell scripting, and fulfillment of the dream of composition


# Looking for common command patterns

## Letting laziness lead the way...

We've seen `csvlook ___ | less -S` quite a few times now for looking at csv data.
So let's give this a name!


# Writing a `csvless` script

Open up a new file in `~/bin/csvless`, and write:

```
#!/bin/bash

csvlook $@ | less -S
```

Here `$@` is a "magic" variable that points to all of the arguments passed to the command.
If you didn't want positional arguments, you can use `$1` or `$2` for the 1st or 2nd (etc.) positional arguments passed to the script.


# Testing our `csvless` script

In the shell:

```
chmod +x ~/bin/csvless

csvless data/sfv.csv

csvcut -c sequence,specimen,species data/sfv.csv | csvless
```


# Another quick example: `csvhead`

Open up a new file in `~/bin/csvhead`, and write:

```
#!/bin/bash

csvlook $@ | head -n 20
```

Again, `chmod +x`, then test it!


# Other things to know for shell scripting & power usage:

* Iteration and arrays: Let us nest or loop over things like different species or locations and process separately
* Conditionals: Let you run tests to determine what code to run
* find/xargs/parallel: Run a command or program in parallel over a sequence of files

Most of this we'll leave to the book...


# Simple iteration

One line in the terminal:

```
for i in $(ls); do echo "My file $i is cool"; done
```

In a file, you can expand this:

```
for i in $(ls)
do
  echo "My file $i is cool"
done
```


# How we might use this?

We could do a series of computations for partitions of our data.
For instance:

```
# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # Create a location outdir, if it doesn't already exist
  loc_outdir="$outdir/$location"
  mkdir -p $loc_outdir
  
  # Do some computations for $location
done
```


# Conditionals

```
#!/bin/bash

if [commands]
then
  [if-statements]
else
  [else-statements]
fi
```


# Conditional example

```
#!/bin/bash

if grep "pattern" some_file.txt > /dev/null
then
  # commands to run if "pattern" is found
  echo "found 'pattern' in 'some_file.txt"
else
  echo "no such pattern found..."
fi
```

Question: Why do we use `> /dev/null` here?


# find/xargs/parallel

There is lots of explanation in the book, so you can learn more there. But to get a brief sense:

* `find`: Lets you compute very specific lists of files, returned to `stdout` separated by lines
* `xargs`: Can take line separated items, and apply them as arguments to some command
    * Good for large collections
    * Can run in parallel
* `parallel`: More robust version of xargs, with better control over parallelization


# Build tools such as `Scons` and `make` offer the following advantages over shell scripts:

* Only rebuild what's needed (very important with long running programs) by tracking:
    * whether data has changed
    * whether commands have changed
* Generally more robust
* Automatically parallelize (sanely)


# Excercise 1

##

How would we add the alignment and tree building we did in the last two sessions to our `build.sh` script?

##

Make the top of our script look like:


```
#!/bin/bash
# ...

inseqs="data/sfv.fasta"

# Alignment
alignment="$outdir/alignment.fasta"
muscle -maxiters 2 -in $inseqs -out $alignment

# Tree
tree="$outdir/tree.nw"
FastTree -nt $alignment > $tree
```


# Excercise 2

## Say we want to do some analyses for each location.

Let's start by creating a separate directory for each.
How would we add location specific metadata using our csvkit tools?


```bash
# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # Create a location outdir, if it doesn't already exist
  loc_outdir="$outdir/$location"
  mkdir -p $loc_outdir

  # Create a subset of the metadata for just that location
  loc_metadata="$loc_outdir/metadata.csv"
  # XXX TODO! Add code here to create $loc_metadata

done
```

##

```bash
# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # Create a location outdir, if it doesn't already exist
  loc_outdir="$outdir/$location"
  mkdir -p $loc_outdir

  # Create a subset of the metadata for just that location
  loc_metadata="$loc_outdir/metadata.csv"
  csvgrep -c location -m $location $metadata > $loc_metadata

done
```


# Excercise 3

## How would we create a phylogenetic tree for each location in our data set?

```bash
# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # ... create location metadata, etc.

  # Create a list of sequences sampled from that location
  loc_sequences="$loc_outdir/sequences"
  csvcut -c sequence $loc_metadata > $loc_sequences

  # TODO:
  # * Subset Ex.1 alignment using `seqmagick convert`
  # * Build a tree from this alignment subset using FastTree
done
```

##

Add the following to the end of our for loop

```bash
# For each location...
locations=($(csvuniq -c location $metadata | tail -n +2))
for location in ${locations[*]}
do
  # ... create location metadata, etc.

  # Create a list of sequences sampled from that location
  loc_sequences="$loc_outdir/sequences"
  csvcut -c sequence $loc_metadata > $loc_sequences

  # Subset our alignment to just that location
  loc_alignment="$loc_outdir/alignment.fasta"
  seqmagick convert --include-from-file $loc_sequences $alignment $loc_alignment

  # Build a location tree
  loc_tree="$loc_outdir/tree.nw"
  FastTree -nt $loc_alignment > $loc_tree

done
```


# Other exercises

* Write a handy little shell script for doing something (for example, printing the last 20 commands entered)


# Reading

Recommended reading:

* Chapter 12

Reading for next class (if you want to read ahead):

* Chapter 5


# Resources

* SCons for bioinformatics post - <http://www.metasoarous.com/scons-for-data-science-and-compbio/>
* Seqmagick - <http://fhcrc.github.io/seqmagick/> for munging sequence data
* Vim for more powerful text editing - (type `vimtutor` at the terminal...)
* Erick's favorite [video](https://www.youtube.com/watch?v=olH-9b3VJfs) about shell scripting, and [website](http://shellhaters.org/)


# This slide intentionally left almost blank...

[Back to homepage](http://fredhutchio.github.io/intro-bioinformatics)


